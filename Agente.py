# Importar bibliotecas necesarias
import streamlit as st
import os
from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from langchain_core.vectorstores.in_memory import InMemoryVectorStore
from langchain_core.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI

# Configurar la clave de API de OpenAI
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# ConfiguraciÃ³n de pÃ¡gina
st.set_page_config(
    page_title="ChatRegulaciÃ³n EnergÃ©tica",
    page_icon="ğŸ“„",
    layout="wide"
)

st.title("ğŸ“„ Asistente RegulaciÃ³n EnergÃ­a - Colombia.")
st.markdown("Carga un documento PDF y haz preguntas sobre su contenido usando IA.")

# Inicializar estado de la sesiÃ³n
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "processed_file" not in st.session_state:
    st.session_state.processed_file = None

def process_pdf(file):
    """Procesar el PDF y configurar los componentes de LangChain"""
    try:
        # Leer el PDF
        pdf_reader = PdfReader(file)
        raw_text = ""
        for page in pdf_reader.pages:
            content = page.extract_text()  # Extraer texto de cada pÃ¡gina
            if content:
                raw_text += content

        # Verificar si se extrajo texto
        if not raw_text:
            raise ValueError("No se pudo extraer texto del PDF")

        # Dividir el texto en fragmentos
        text_splitter = CharacterTextSplitter(
            separator="\n",  # Separador de lÃ­neas
            chunk_size=800,  # TamaÃ±o mÃ¡ximo de fragmentos
            chunk_overlap=200,  # Solapamiento entre fragmentos
            length_function=len,  # FunciÃ³n para medir longitud
        )
        texts = text_splitter.split_text(raw_text)

        # Crear embeddings y almacÃ©n vectorial
        embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")  # Explicitly specify model, no proxies
        document_search = FAISS.from_texts(texts, embeddings)  # Usar FAISS para mejor rendimiento

        # Definir rol y plantilla de prompt
        role_description = (
            "Eres un asistente de IA especializado en anÃ¡lisis de la regulaciÃ³n del mercado "
            "de energÃ­a en Colombia. Proporciona respuestas precisas y detalladas basadas en el documento proporcionado."
        )
        QA_CHAIN_PROMPT = PromptTemplate.from_template(
            role_description + "\n\nPregunta: {question}\nContexto: {context}\nRespuesta:"
        )

        # Inicializar el modelo de lenguaje
        llm = ChatOpenAI(temperature=0.3, model_name="gpt-3.5-turbo")

        # Configurar memoria del chat
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )

        # Crear cadena conversacional
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=document_search.as_retriever(),
            memory=memory,
            combine_docs_chain_kwargs={"prompt": QA_CHAIN_PROMPT}
        )
        return qa_chain
    except Exception as e:
        st.error(f"Error al procesar el PDF: {str(e)}")
        return None

# Sidebar para configuraciÃ³n
with st.sidebar:
    st.header("ğŸ“‹ ConfiguraciÃ³n")
    
    # Componente para cargar archivo PDF
    uploaded_file = st.file_uploader(
        "Carga un archivo PDF",
        type=["pdf"],
        help="Sube un documento PDF para analizarlo con IA"
    )
    
    # BotÃ³n para limpiar chat
    if st.button("ğŸ—‘ï¸ Limpiar Chat"):
        st.session_state.chat_history = []
        st.session_state.qa_chain = None
        st.session_state.processed_file = None
        st.rerun()

# Procesar el PDF cuando se carga (solo si es un archivo nuevo)
if uploaded_file and (st.session_state.processed_file != uploaded_file.name):
    with st.spinner("ğŸ”„ Procesando PDF..."):
        st.session_state.qa_chain = process_pdf(uploaded_file)
        st.session_state.processed_file = uploaded_file.name
    
    if st.session_state.qa_chain:
        st.success("âœ… Â¡PDF procesado correctamente! Ahora puedes hacer preguntas.")
    else:
        st.error("âŒ No se pudo procesar el PDF. Verifica el archivo e intenta de nuevo.")

# Ãrea principal de chat
col1, col2 = st.columns([3, 1])

with col1:
    st.subheader("ğŸ’¬ ConversaciÃ³n")
    
    # Contenedor para el chat con scroll
    chat_container = st.container()
    
    with chat_container:
        # Mostrar historial de chat
        for i, message in enumerate(st.session_state.chat_history):
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.markdown(f"**TÃº:** {message['content']}")
            else:
                with st.chat_message("assistant"):
                    st.markdown(f"**Asistente:** {message['content']}")

with col2:
    if st.session_state.qa_chain:
        st.success("ğŸ“„ PDF cargado")
        st.info(f"ğŸ“Š {len(st.session_state.chat_history)} mensajes")
    else:
        st.warning("âš ï¸ Sin PDF")

# Entrada para nueva pregunta
if st.session_state.qa_chain:
    with st.form(key="question_form", clear_on_submit=True):
        user_input = st.text_area(
            "Haz una pregunta sobre el PDF:",
            height=100,
            placeholder="Ejemplo: Â¿CuÃ¡les son los principales puntos de la regulaciÃ³n CREG 071-2006?"
        )
        
        col1, col2, col3 = st.columns([1, 1, 4])
        with col1:
            submit_button = st.form_submit_button("ğŸ“¤ Enviar", use_container_width=True)
        with col2:
            clear_button = st.form_submit_button("ğŸ—‘ï¸ Limpiar", use_container_width=True)

    # Procesar la pregunta
    if submit_button and user_input.strip():
        # Agregar pregunta al historial
        st.session_state.chat_history.append({
            "role": "user", 
            "content": user_input.strip()
        })

        # Generar respuesta
        with st.spinner("ğŸ¤” Generando respuesta..."):
            try:
                response = st.session_state.qa_chain({
                    "question": user_input.strip()
                })
                
                answer = response.get("answer", "No se pudo generar una respuesta.")
                
                # Agregar respuesta al historial
                st.session_state.chat_history.append({
                    "role": "assistant", 
                    "content": answer
                })
                
                # Recargar para mostrar la nueva conversaciÃ³n
                st.rerun()
                
            except Exception as e:
                st.error(f"âŒ Error al generar respuesta: {str(e)}")
                
    elif submit_button and not user_input.strip():
        st.warning("âš ï¸ Por favor, ingresa una pregunta.")
        
    elif clear_button:
        st.session_state.chat_history = []
        st.rerun()
        
else:
    st.info("ğŸ‘† **Primero carga un archivo PDF usando el panel lateral**")
    
    # Ejemplos de preguntas
    st.subheader("ğŸ’¡ Ejemplos de preguntas que puedes hacer:")
    examples = [
        "Â¿CuÃ¡l es el objetivo principal de esta regulaciÃ³n?",
        "Â¿QuÃ© entidades estÃ¡n involucradas en este proceso?",
        "Â¿CuÃ¡les son las obligaciones principales establecidas?",
        "Â¿QuÃ© sanciones se contemplan?",
        "Â¿CuÃ¡ndo entra en vigencia esta regulaciÃ³n?"
    ]
    
    for example in examples:
        st.markdown(f"â€¢ {example}")

# Footer
st.markdown("---")
